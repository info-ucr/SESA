{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import argparse\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn # import modules\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid\n",
    "from models_v4.Update import LocalTrain\n",
    "from models_v4.Nets import MLP\n",
    "from models_v4.Fed import FedAvg\n",
    "from models_v4.test import test_acc\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad\n",
    "\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 400    #\"rounds of training\"\n",
    "    num_users = 100  # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep=1 #\"the number of local epochs: E\"\n",
    "    local_bs=10 #\"local batch size: B\"\n",
    "    bs=128 #\"test batch size\"\n",
    "    lr=0.001 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    weight_decay = 5e-4\n",
    "    opt = 'ADAM'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'mlp'\n",
    "    \n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=0\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    \n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    model='mlp'\n",
    "    q=20\n",
    "    f_size=32\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cpu\")\n",
    "args.device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_gen,FedAvg_gradient, weight_vectorization_gen2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non iid dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "def phiQ(p,sc, q, w):\n",
    "    w_cap = w #[:,0]\n",
    "    #w_cap=[item[0] if isinstance(item,list) and len(item)>0 else item for item in w_cap]\n",
    "    #w_cap=np.array(w_cap)\n",
    "    #print(\"w_cap_first\")\n",
    "    #print(w_cap)\n",
    "    w_cap= sc*w_cap\n",
    "    v=np.floor(q*w_cap)\n",
    "    one=np.ones(len(w))\n",
    "    r=np.random.uniform(0,1,len(w_cap))\n",
    "    temp=(one.T+np.sign(q*w_cap-v-r))*np.sign(q*w_cap-v-r)\n",
    "    #j=np.sign(w_cap-v-r)\n",
    "    #print(\"jjj\")\n",
    "    #print(j)\n",
    "    #temp= (1/q)*(1/2)*temp\n",
    "    temp=(1/2)*temp\n",
    "    #print(\"temp\")\n",
    "    #print(temp)\n",
    "    #w_cap= (1/q)*v + temp\n",
    "    w_cap=(1/q)*(v+temp)\n",
    "    w_cap=q*w_cap\n",
    "    #w_cap=w_cap+ (1/2)*p*(-np.sign(w_cap)+one.T)*(-np.sign(w_cap))\n",
    "    w_cap=w_cap+ (1/2)*(p-5)*(-np.sign(w_cap)+one.T)*(-np.sign(w_cap))\n",
    "    #print(\"w_cap_last\")\n",
    "    #print(w_cap)\n",
    "    del temp\n",
    "    del one\n",
    "    del v\n",
    "    del r\n",
    "    return w_cap\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('/data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('/data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "        print('iid dataset')\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "        print(\"non iid dataset\")\n",
    "\n",
    "else:\n",
    "    exit('Error: dataset not found')\n",
    "img_size = dataset_train[0][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNumpyFlatArray(self):\n",
    "        return self.flat\n",
    "\n",
    "\n",
    "def updateFromNumpyFlatArray(self, arr):\n",
    "    self.flat = arr\n",
    "    start = 0\n",
    "    new_glob = OrderedDict()\n",
    "    for k in self.w_glob.keys():\n",
    "        size = 1\n",
    "        for dim in self.w_glob[k].shape:\n",
    "            size *= dim\n",
    "        shaped = np.reshape(arr[start : start + size].copy(), self.w_glob[k].shape)\n",
    "        new_glob[k] = torch.from_numpy(shaped)\n",
    "        start = start + size\n",
    "    self.w_glob = new_glob\n",
    "    self.net_glob.load_state_dict(self.w_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "    if args.iid:\n",
    "        print(\"iid dataset\")\n",
    "        dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        exit('Error: only consider IID setting in CIFAR10')\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta=[1,1/2,1/4,1/8,1/16]\n",
    "# #beta=[1,1,1,1,1]\n",
    "# beta=[1,1,1/2,1/2,1/4]\n",
    "# beta=[1,1/2,1/4,1/8]\n",
    "# comp=[]\n",
    "# i=0\n",
    "# for j in range(args.num_users):\n",
    "#     comp.append(beta[i])\n",
    "#     if (j>0 and j%25==0):\n",
    "#         i+=1\n",
    "beta=[1,1/2,1/4,1/8,1/16]\n",
    "beta=[1,1,1/2,1/2,1/4]\n",
    "comp=[]\n",
    "i=0\n",
    "for j in range(args.num_users):\n",
    "    comp.append(beta[i])\n",
    "    if (j>0 and j%20==0):\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n"
     ]
    }
   ],
   "source": [
    "print(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_users[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_users[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_users[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FedAvg with A=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_lr = 1e-4\n",
    "local_steps = 1\n",
    "use_updates = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        seed=123\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        #torch.nn.init.xavier_uniform_(m.bias.data)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "MLP(\n",
      "  (layer_input): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer_hidden): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer_input): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (layer_hidden): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_gen,FedAvg_gradient\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "# build model\n",
    "\n",
    "if args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    print(len_in)\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error:model not found')\n",
    "print(net_glob)\n",
    "net_glob.apply(weights_init)\n",
    "net_glob.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(w_glob['layer_hidden.weight'][:,s:s+y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_1=[]\n",
    "for i in range(args.num_users):\n",
    "    s_1.append(np.random.randint(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_local_model( w_hold,w_local, y,s):\n",
    "    if (s+y>200):\n",
    "            w_glob_hold['layer_input.weight'][0:y-(200-s),:]=w_local['layer_input.weight'][0:y-(200-s),:]\n",
    "            w_glob_hold['layer_input.weight'][s:200,:]=w_local['layer_input.weight'][-(200-s):,:]\n",
    "            #combined=torch.concatenate((first,middle),axis=0)\n",
    "    \n",
    "    else:\n",
    "            w_glob_hold['layer_input.weight'][s:s+y,:]= w_local['layer_input.weight']\n",
    "            #combined=middle\n",
    "    #w_glob['layer_input.weight']=combined\n",
    "    if (s+y>200):\n",
    "            w_glob_hold['layer_input.bias'][0:y-(200-s)]=w_local['layer_input.bias'][0:y-(200-s)]\n",
    "            w_glob_hold['layer_input.bias'][s:200]=w_local['layer_input.bias'][-(200-s):]\n",
    "            #combined=torch.concatenate((first,middle),axis=0)\n",
    "    \n",
    "    else:\n",
    "            w_glob_hold['layer_input.bias'][s:s+y] = w_local['layer_input.bias']\n",
    "           \n",
    "    #w_glob['layer_input.bias']=combined\n",
    "        \n",
    "    if (s+y>200):\n",
    "            w_glob_hold['layer_hidden.weight'][:,0:y-(200-s)]=w_local['layer_hidden.weight'][:,0:y-(200-s)]\n",
    "            w_glob_hold['layer_hidden.weight'][:,s:200]=w_local['layer_hidden.weight'][:,-(200-s):]\n",
    "            #combined=torch.concatenate((first,middle),axis=1)\n",
    "    else:\n",
    "            w_glob_hold['layer_hidden.weight'][:,s:s+y]=w_local['layer_hidden.weight']\n",
    "    w_glob_hold['layer_hidden.bias']=w_local['layer_hidden.bias']\n",
    "            #combined=middle\n",
    "    #w_glob['layer_hidden.weight']=combined\n",
    "    return w_glob_hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109, 126, 66, 98, 17, 83, 106, 123, 57, 96, 113, 126, 47, 73, 32, 174, 111, 153, 83, 78, 164, 96, 68, 49, 55, 195, 2, 84, 39, 66, 84, 47, 189, 176, 135, 105, 99, 124, 92, 180, 102, 97, 118, 94, 155, 34, 76, 168, 131, 106, 69, 64, 75, 162, 58, 138, 22, 146, 15, 155, 158, 180, 70, 109, 115, 154, 134, 14, 103, 182, 199, 129, 43, 186, 101, 183, 25, 178, 56, 49, 12, 18, 1, 51, 172, 117, 48, 56, 177, 86, 3, 67, 139, 149, 103, 98, 3, 139, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 784])\n",
      "torch.Size([200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10])\n",
      "torch.Size([200, 784])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([200])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10, 200])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_cifar,FedAvg_gradient, weight_vectorization_gen, weight_vectorization_gen2\n",
    "#net_glob = LeNet10().to(args.device)\n",
    "#net_glob.train()\n",
    "args.lr=0.0005\n",
    "import torchvision.models as models\n",
    "dev=torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "net_glob.apply(weights_init)\n",
    "net_glob.train()\n",
    "#net_glob.load_state_dict(w_glob)\n",
    "net_glob=net_glob.to(dev)\n",
    "net_glob.train()\n",
    "# copy weights\n",
    "#net_glob=torch.hub.load('pytorch/vision:v0.10.0','resnet18',pretrained=False)\n",
    "#net_glob.eval()\n",
    "w_glob = net_glob.state_dict()\n",
    "g, dim = weight_vectorization_gen(w_glob)\n",
    "\n",
    "w_glob=net_glob.state_dict()\n",
    "# print(w_glob)\n",
    "# print(w_glob.keys())\n",
    "for k in w_glob.keys():\n",
    "    print(w_glob[k].shape)\n",
    "net_glob_original=copy.deepcopy(net_glob)\n",
    "net_glob_original.to(dev)\n",
    "w_glob_original=copy.deepcopy(w_glob)\n",
    "w_glob_hold=net_glob_original.state_dict()\n",
    "#print(w_glob_hold)\n",
    "for h in w_glob_hold.keys():\n",
    "    print(w_glob_hold[h].shape)\n",
    "    #print(w_glob[h])\n",
    "    w_glob_hold[h]=torch.zeros(w_glob_hold[h].shape)\n",
    "    print(type(w_glob[h]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(w_glob_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no. 0\n",
      "accuracy array\n",
      "9.5600004196167\n",
      "train loss\n",
      "0.4728033218823663\n",
      "iteration no. 1\n",
      "accuracy array\n",
      "24.3799991607666\n",
      "train loss\n",
      "0.3634827059504733\n",
      "iteration no. 2\n",
      "accuracy array\n",
      "31.809999465942383\n",
      "train loss\n",
      "0.3067583966095929\n",
      "iteration no. 3\n",
      "accuracy array\n",
      "40.7599983215332\n",
      "train loss\n",
      "0.27722716654862967\n",
      "iteration no. 4\n",
      "accuracy array\n",
      "45.22999954223633\n",
      "train loss\n",
      "0.2496264646833257\n",
      "iteration no. 5\n",
      "accuracy array\n",
      "48.83000183105469\n",
      "train loss\n",
      "0.23446507055779225\n",
      "iteration no. 6\n",
      "accuracy array\n",
      "56.02000045776367\n",
      "train loss\n",
      "0.22515978208831086\n",
      "iteration no. 7\n",
      "accuracy array\n",
      "56.209999084472656\n",
      "train loss\n",
      "0.20129812622766016\n",
      "iteration no. 8\n",
      "accuracy array\n",
      "57.150001525878906\n",
      "train loss\n",
      "0.20218974664412864\n",
      "iteration no. 9\n",
      "accuracy array\n",
      "54.91999816894531\n",
      "train loss\n",
      "0.19640451623996608\n",
      "iteration no. 10\n",
      "accuracy array\n",
      "58.7599983215332\n",
      "train loss\n",
      "0.19595456757933924\n",
      "iteration no. 11\n",
      "accuracy array\n",
      "60.04999923706055\n",
      "train loss\n",
      "0.19239409458913548\n",
      "iteration no. 12\n",
      "accuracy array\n",
      "65.5\n",
      "train loss\n",
      "0.19103149370384467\n",
      "iteration no. 13\n",
      "accuracy array\n",
      "67.33000183105469\n",
      "train loss\n",
      "0.18618223719672228\n",
      "iteration no. 14\n",
      "accuracy array\n",
      "69.29000091552734\n",
      "train loss\n",
      "0.176760437130349\n",
      "iteration no. 15\n",
      "accuracy array\n",
      "69.06999969482422\n",
      "train loss\n",
      "0.1790287821295598\n",
      "iteration no. 16\n",
      "accuracy array\n",
      "68.86000061035156\n",
      "train loss\n",
      "0.17300931758647758\n",
      "iteration no. 17\n",
      "accuracy array\n",
      "69.19999694824219\n",
      "train loss\n",
      "0.16784967369725542\n",
      "iteration no. 18\n",
      "accuracy array\n",
      "71.33000183105469\n",
      "train loss\n",
      "0.17429776087575904\n",
      "iteration no. 19\n",
      "accuracy array\n",
      "69.83000183105469\n",
      "train loss\n",
      "0.1707878020685203\n",
      "iteration no. 20\n",
      "accuracy array\n",
      "72.61000061035156\n",
      "train loss\n",
      "0.161350376603536\n",
      "iteration no. 21\n",
      "accuracy array\n",
      "69.98999786376953\n",
      "train loss\n",
      "0.16272646565068288\n",
      "iteration no. 22\n",
      "accuracy array\n",
      "68.08999633789062\n",
      "train loss\n",
      "0.1575971552932189\n",
      "iteration no. 23\n",
      "accuracy array\n",
      "71.11000061035156\n",
      "train loss\n",
      "0.16068231143606962\n",
      "iteration no. 24\n",
      "accuracy array\n",
      "71.12999725341797\n",
      "train loss\n",
      "0.1560277315487027\n",
      "iteration no. 25\n",
      "accuracy array\n",
      "70.83000183105469\n",
      "train loss\n",
      "0.15383846512652533\n",
      "iteration no. 26\n",
      "accuracy array\n",
      "68.36000061035156\n",
      "train loss\n",
      "0.15481646696524376\n",
      "iteration no. 27\n",
      "accuracy array\n",
      "69.5\n",
      "train loss\n",
      "0.16026426208407343\n",
      "iteration no. 28\n",
      "accuracy array\n",
      "71.22000122070312\n",
      "train loss\n",
      "0.15278794237148988\n",
      "iteration no. 29\n",
      "accuracy array\n",
      "69.3499984741211\n",
      "train loss\n",
      "0.15679721627254473\n",
      "iteration no. 30\n",
      "accuracy array\n",
      "68.97000122070312\n",
      "train loss\n",
      "0.1542704402631178\n",
      "iteration no. 31\n",
      "accuracy array\n",
      "71.62000274658203\n",
      "train loss\n",
      "0.15427923995864296\n",
      "iteration no. 32\n",
      "accuracy array\n",
      "71.66000366210938\n",
      "train loss\n",
      "0.1442965220164229\n",
      "iteration no. 33\n",
      "accuracy array\n",
      "72.30000305175781\n",
      "train loss\n",
      "0.14908861628478853\n",
      "iteration no. 34\n",
      "accuracy array\n",
      "70.66000366210938\n",
      "train loss\n",
      "0.15511783180206645\n",
      "iteration no. 35\n",
      "accuracy array\n",
      "70.33999633789062\n",
      "train loss\n",
      "0.14979671942506498\n",
      "iteration no. 36\n",
      "accuracy array\n",
      "71.26000213623047\n",
      "train loss\n",
      "0.15642225237418694\n",
      "iteration no. 37\n",
      "accuracy array\n",
      "69.0999984741211\n",
      "train loss\n",
      "0.1523572545440797\n",
      "iteration no. 38\n",
      "accuracy array\n",
      "69.44000244140625\n",
      "train loss\n",
      "0.15654244905657408\n",
      "iteration no. 39\n",
      "accuracy array\n",
      "70.94999694824219\n",
      "train loss\n",
      "0.15775056345461558\n",
      "iteration no. 40\n",
      "accuracy array\n",
      "72.47000122070312\n",
      "train loss\n",
      "0.1554770218457182\n",
      "iteration no. 41\n",
      "accuracy array\n",
      "70.30999755859375\n",
      "train loss\n",
      "0.15669921819455726\n",
      "iteration no. 42\n",
      "accuracy array\n",
      "69.4000015258789\n",
      "train loss\n",
      "0.15663900684393672\n",
      "iteration no. 43\n",
      "accuracy array\n",
      "65.4000015258789\n",
      "train loss\n",
      "0.15093100820754166\n",
      "iteration no. 44\n",
      "accuracy array\n",
      "67.2300033569336\n",
      "train loss\n",
      "0.16024050977672857\n",
      "iteration no. 45\n",
      "accuracy array\n",
      "72.94999694824219\n",
      "train loss\n",
      "0.15273265671681194\n",
      "iteration no. 46\n",
      "accuracy array\n",
      "73.8499984741211\n",
      "train loss\n",
      "0.15357790739345817\n",
      "iteration no. 47\n",
      "accuracy array\n",
      "70.62999725341797\n",
      "train loss\n",
      "0.15653282800839902\n",
      "iteration no. 48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training\n",
    "loss_train = []\n",
    "loss_test_arr = []\n",
    "acc_test_arr = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "m_local=[]\n",
    "d=11699132 #resnet18\n",
    "d= 11173962\n",
    "d=11183582\n",
    "d=21840\n",
    "#d= 11699132\n",
    "#d=62006\n",
    "iter_no=2000\n",
    "avg=[]\n",
    "error=[]\n",
    "idxs_users=range(0,args.num_users)\n",
    "    #print(len(idxs_users))\n",
    "user_no=args.num_users\n",
    "updated=[]\n",
    "a=[]\n",
    "for user in idxs_users:\n",
    "        #print(user)\n",
    "    updated.append([])  \n",
    "    a.append([])   \n",
    "model_diff=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#net_glob.zero_grad()\n",
    "input_gradient=[]\n",
    "\n",
    "user_no=args.num_users\n",
    "\n",
    "select=round(0.9*user_no)\n",
    "\n",
    "K_local=round(1*d)\n",
    "loss_train=[]    \n",
    "#net_glob.zero_grad()   \n",
    "for iter in range(iter_no): #args.epochs\n",
    "    print(\"iteration no.\",iter)\n",
    "#     if (iter>=10):\n",
    "#         args.lr=0.0001\n",
    "    if (iter>=500):\n",
    "        args.lr=args.lr /10\n",
    "    m_local=[]\n",
    "    f=[]\n",
    "        #T=[]\n",
    "    w_locals, loss_locals,diff_locals,grad_locals = [], [],[],[]\n",
    "    m = 10\n",
    "    updated=[]\n",
    "    model_diff=[]\n",
    "    grad_vect=[]\n",
    "    prev=[]\n",
    "    error=[]\n",
    "    grad_vect_quant=[]\n",
    "    grad_vect_quant2=[]\n",
    "    grad_vect_send=[]\n",
    "    grad_vect_send2=[]\n",
    "    store_grad=[]\n",
    "    location_local=[]\n",
    "    rand=np.random.choice(idxs_users,user_no,replace=False)\n",
    "    rand=idxs_users\n",
    "    np.random.seed(iter)\n",
    "    rand=np.random.choice(idxs_users, select, replace=False)\n",
    "    rand=np.sort(rand)\n",
    "    loss_train_user=[]\n",
    "    for i in range(args.num_users):\n",
    "        updated.append([])\n",
    "        model_diff.append([])\n",
    "        grad_vect.append([])\n",
    "        prev.append([])\n",
    "        grad_vect_send.append([])\n",
    "        error.append(np.zeros(d))\n",
    "        grad_vect_quant.append([])\n",
    "    \n",
    "    \n",
    "    for user in rand : #rand: #idxs_users:\n",
    "        #print(user)\n",
    "        \n",
    "        w_glob = net_glob_original.state_dict()\n",
    "        \n",
    "        s_1[user]=s_1[user]%200\n",
    "        s=s_1[user]\n",
    "#print(w_glob['layer_input.weight'][:,s:s+10])\n",
    "        \n",
    "\n",
    "        y=int(comp[user]*200)\n",
    "\n",
    "        if (s+y>200):\n",
    "            first=w_glob['layer_input.weight'][0:y-(200-s),:]\n",
    "            #print(first.shape)\n",
    "            middle=w_glob['layer_input.weight'][s:200,:]\n",
    "            #print(middle.shape)\n",
    "            combined=torch.concatenate((first,middle),axis=0)\n",
    "            #print(combined.shape)\n",
    "    \n",
    "        else:\n",
    "            middle=w_glob['layer_input.weight'][s:s+y,:]\n",
    "            combined=middle\n",
    "        w_glob['layer_input.weight']=combined\n",
    "        if (s+y>200):\n",
    "            first=w_glob['layer_input.bias'][0:y-(200-s)]\n",
    "            middle=w_glob['layer_input.bias'][s:200]\n",
    "            combined=torch.concatenate((first,middle),axis=0)\n",
    "    \n",
    "        else:\n",
    "            middle=w_glob['layer_input.bias'][s:s+y]\n",
    "            combined=middle\n",
    "        w_glob['layer_input.bias']=combined\n",
    "        \n",
    "        if (s+y>200):\n",
    "            first=w_glob['layer_hidden.weight'][:,0:y-(200-s)]\n",
    "            middle=w_glob['layer_hidden.weight'][:,s:200]\n",
    "            combined=torch.concatenate((first,middle),axis=1)\n",
    "        else:\n",
    "            middle=w_glob['layer_hidden.weight'][:,s:s+y]\n",
    "            combined=middle\n",
    "        w_glob['layer_hidden.weight']=combined\n",
    "        \n",
    "        #print(w_glob)\n",
    "        \n",
    "        net_glob = MLP(dim_in=len_in, dim_hidden=y, dim_out=args.num_classes).to(args.device)\n",
    "        net_glob.train()\n",
    "        net_glob.to(dev)\n",
    "        \n",
    "        net_glob.load_state_dict(w_glob)\n",
    "        s_1[user]=(s_1[user]+1)%200\n",
    "        \n",
    "        prev[user]=copy.deepcopy(w_glob)\n",
    "        local = LocalTrain(args=args, dataset=dataset_train, idxs=dict_users[user])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "#         input_gradient=inversefed.reconstruction_algorithms.loss_steps(net_glob, ground_truth[user], labels[user], \n",
    "#                                                         lr=local_lr, local_steps=local_steps,\n",
    "#                                                                    use_updates=use_updates)\n",
    "        \n",
    "        loss_train_user.append(loss)\n",
    "    \n",
    "        model_diff=FedSubstract(w,prev[user])\n",
    "        #print(model_diff)\n",
    "        \n",
    "#         g, dim = weight_vectorization_gen(model_diff)\n",
    "#         print(len(g))\n",
    "        \n",
    "        model_diff=update_local_model(w_glob_hold, model_diff, y,s)\n",
    "        #print(model_diff2)\n",
    "        \n",
    "        g, dim = weight_vectorization_gen(model_diff)\n",
    "        #print(len(g))\n",
    "        g=g[:,0] #+error[user]\n",
    "        grad1=abs(np.array(g))\n",
    "        \n",
    "        grad_vect[user]= g #np.multiply(g,mask)\n",
    "        scale=1/(1*0.9*args.num_users)\n",
    "        grad_vect_quant[user]= scale*grad_vect[user] #phiQ(np.power(2,args.f_size),scale,2**args.q,grad_vect[user])\n",
    "        #error[user]=g-grad_vect[user]\n",
    "        grad_locals.append(grad_vect_quant[user])\n",
    "        del g\n",
    "        #print(user)\n",
    "    grad_avg=sum(grad_locals) #/len(grad_locals)\n",
    "    loss_train.append(sum(loss_train_user)/len(loss_train_user))\n",
    "    grad_avg=sum(grad_locals) # taking the average of masked gradients\n",
    "    \n",
    "    grad_avg_correct = grad_avg #np.zeros_like(grad_avg)\n",
    "    \n",
    "    p=np.power(2,args.f_size)-5\n",
    "\n",
    "    count=0\n",
    "    w_glob_prev=copy.deepcopy(w_glob_original)\n",
    "    flat=[]\n",
    "    #conver\n",
    "    for i in range(len(w_glob.keys())): # 4 layers in parameter\n",
    "        flat.append([])\n",
    "\n",
    "    for h in w_glob_prev.keys():\n",
    "        s=list(w_glob_original[h].shape)\n",
    "        if (len(s)==0):\n",
    "            new=np.array(0)\n",
    "            grad_avg_correct=np.delete(grad_avg_correct,np.s_[0])\n",
    "        else:\n",
    "            z=np.prod(list(w_glob_original[h].shape))\n",
    "            flat[count]=grad_avg_correct[0:z] # taking out the vector for the specified layer\n",
    "            grad_avg_correct=np.delete(grad_avg_correct,np.s_[0:z]) # deleting that vector from decoded after taking out\n",
    "             \n",
    "            new=flat[count].reshape(list(w_glob_original[h].shape)) #reshaping back to the marix\n",
    "              \n",
    "        w_glob_original[h]=torch.from_numpy(new) #converting the matrix to a tensor\n",
    "            #print(w_glob[cluster_no][h].shape)\n",
    "        count=count+1\n",
    "    global_diff = w_glob_original\n",
    "    #print(w_glob)\n",
    "    w_glob_original=FedAdd(w_glob_prev,global_diff)\n",
    "    # update global weights\n",
    "    #global_diff = w_glob\n",
    "    #print(w_glob)\n",
    "    #w_glob=FedAdd(w_glob_prev,global_diff)\n",
    "    \n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob_original.load_state_dict(w_glob_original)\n",
    "    \n",
    "    del w_glob_prev\n",
    "    del grad_locals\n",
    "    del grad_avg\n",
    "    del flat\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # print loss\n",
    "    #loss_avg = np.nan_to_num(sum(loss_locals) / len(loss_locals))\n",
    "    \n",
    "    #loss_train.append(float(loss_avg))\n",
    "    \n",
    "    acc_test, loss_test = test_acc(net_glob, dataset_test, args)\n",
    "    acc_test_arr.append(float(acc_test))\n",
    "    loss_test_arr.append(loss_test)\n",
    "    if iter % 1 ==0:\n",
    "        #print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    " \n",
    "        print(\"accuracy array\")\n",
    "        print(acc_test_arr[iter])\n",
    "        print(\"train loss\")\n",
    "        print(loss_train[iter])       \n",
    "        \n",
    "        \n",
    "   \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_diff['layer_hidden.weight'])\n",
    "#print(model_diff2['layer_hidden.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in w_glob.keys():\n",
    "    print(w_glob[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad_avg_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_glob_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in model_diff.keys():\n",
    "    print(model_diff[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_1[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "step=1000\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "matplotlib.rc('xtick', labelsize=18) \n",
    "matplotlib.rc('ytick', labelsize=18) \n",
    "#plt.plot(range(len(Cluster0_minor1)), Cluster0_minor1,label=\"users=2,5,5,6,7\")\n",
    "plt.plot(range(len(acc_test_arr[0:step])), acc_test_arr[0:step],label=\"without sparsification\")\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.xlabel('# Global Rounds')\n",
    "#plt.legend()\n",
    "#plt.figure(figsize=(6,5), dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(acc_test_arr[0:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net_glob.state_dict(),\"model_cnn.pt\")\n",
    "#for k in w_glob.keys():\n",
    "#print(w_glob['conv1.weight'].to(torch.device(\"cuda\"))-w_glob['conv1.weight'].to(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
