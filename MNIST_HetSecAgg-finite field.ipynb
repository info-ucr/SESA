{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import argparse\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn # import modules\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid\n",
    "from models_v4.Update import LocalTrain\n",
    "from models_v4.Nets import MLP\n",
    "from models_v4.Fed import FedAvg\n",
    "\n",
    "from models_v4.test import test_acc\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 400    #\"rounds of training\"\n",
    "    num_users = 100  # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep=1 #\"the number of local epochs: E\"\n",
    "    local_bs=10 #\"local batch size: B\"\n",
    "    bs=128 #\"test batch size\"\n",
    "    lr=0.001 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    weight_decay = 5e-4\n",
    "    opt = 'ADAM'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'mlp'\n",
    "    \n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=0\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=3#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    \n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    model='mlp'\n",
    "    q=20\n",
    "    f_size=32\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cpu\")\n",
    "args.device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_gen,FedAvg_gradient, weight_vectorization_gen2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non iid dataset\n"
     ]
    }
   ],
   "source": [
    "def phiQ(p,sc, q, w):\n",
    "    w_cap = w #[:,0]\n",
    "    #w_cap=[item[0] if isinstance(item,list) and len(item)>0 else item for item in w_cap]\n",
    "    #w_cap=np.array(w_cap)\n",
    "    #print(\"w_cap_first\")\n",
    "    #print(w_cap)\n",
    "    w_cap= sc*w_cap\n",
    "    v=np.floor(q*w_cap)\n",
    "    one=np.ones(len(w))\n",
    "    r=np.random.uniform(0,1,len(w_cap))\n",
    "    temp=(one.T+np.sign(q*w_cap-v-r))*np.sign(q*w_cap-v-r)\n",
    "    #j=np.sign(w_cap-v-r)\n",
    "    #print(\"jjj\")\n",
    "    #print(j)\n",
    "    #temp= (1/q)*(1/2)*temp\n",
    "    temp=(1/2)*temp\n",
    "    #print(\"temp\")\n",
    "    #print(temp)\n",
    "    #w_cap= (1/q)*v + temp\n",
    "    w_cap=(1/q)*(v+temp)\n",
    "    w_cap=q*w_cap\n",
    "    #w_cap=w_cap+ (1/2)*p*(-np.sign(w_cap)+one.T)*(-np.sign(w_cap))\n",
    "    w_cap=w_cap+ (1/2)*(p-5)*(-np.sign(w_cap)+one.T)*(-np.sign(w_cap))\n",
    "    #print(\"w_cap_last\")\n",
    "    #print(w_cap)\n",
    "    del temp\n",
    "    del one\n",
    "    del v\n",
    "    del r\n",
    "    return w_cap\n",
    "\n",
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('/data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('/data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "        print('iid dataset')\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "        print(\"non iid dataset\")\n",
    "\n",
    "else:\n",
    "    exit('Error: dataset not found')\n",
    "img_size = dataset_train[0][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNumpyFlatArray(self):\n",
    "        return self.flat\n",
    "\n",
    "\n",
    "def updateFromNumpyFlatArray(self, arr):\n",
    "    self.flat = arr\n",
    "    start = 0\n",
    "    new_glob = OrderedDict()\n",
    "    for k in self.w_glob.keys():\n",
    "        size = 1\n",
    "        for dim in self.w_glob[k].shape:\n",
    "            size *= dim\n",
    "        shaped = np.reshape(arr[start : start + size].copy(), self.w_glob[k].shape)\n",
    "        new_glob[k] = torch.from_numpy(shaped)\n",
    "        start = start + size\n",
    "    self.w_glob = new_glob\n",
    "    self.net_glob.load_state_dict(self.w_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=[1,1/2,1/4,1/8,1/16]\n",
    "beta=[1,1,1/2,1/2,1/4]\n",
    "comp=[]\n",
    "i=0\n",
    "for j in range(args.num_users):\n",
    "    comp.append(beta[i])\n",
    "    if (j>0 and j%20==0):\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n"
     ]
    }
   ],
   "source": [
    "print(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_users[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_users[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_users[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FedAvg with A=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_lr = 1e-4\n",
    "local_steps = 1\n",
    "use_updates = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        seed=123\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        nn.init.xavier_uniform(m.weight.data, nn.init.calculate_gain('relu'))\n",
    "        #nn.init.xavier_uniform(m.bias.data)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "MLP(\n",
      "  (layer_input): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer_hidden): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer_input): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (layer_hidden): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_gen,FedAvg_gradient\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "if args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    print(len_in)\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error:model not found')\n",
    "print(net_glob)\n",
    "net_glob.apply(weights_init)\n",
    "net_glob.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_local_model( w_hold,w_local, sub):\n",
    "    count=0\n",
    "    for i in sub:\n",
    "            w_glob_hold['layer_input.weight'][int(50*i):int(50*(i+1)),:]=w_local['layer_input.weight'][count*50:(count+1)*50,:]\n",
    "            \n",
    "            #combined=torch.concatenate((first,middle),axis=0)\n",
    "    \n",
    "            w_glob_hold['layer_input.bias'][int(50*i):int(50*(i+1))]=w_local['layer_input.bias'][count*50:(count+1)*50]\n",
    "            \n",
    "    \n",
    "    \n",
    "            w_glob_hold['layer_hidden.weight'][:,int(50*i):int(50*(i+1))]=w_local['layer_hidden.weight'][:,count*50:(count+1)*50]\n",
    "            \n",
    "    \n",
    "            w_glob_hold['layer_hidden.bias']=w_local['layer_hidden.bias']\n",
    "            count+=1\n",
    "            #combined=middle\n",
    "    #w_glob['layer_hidden.weight']=combined\n",
    "    return w_glob_hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 784])\n",
      "torch.Size([200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10])\n",
      "torch.Size([200, 784])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([200])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10, 200])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_cifar,FedAvg_gradient, weight_vectorization_gen, weight_vectorization_gen2\n",
    "#net_glob = LeNet10().to(args.device)\n",
    "#net_glob.train()\n",
    "args.lr=0.0005\n",
    "import torchvision.models as models\n",
    "dev=torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "net_glob.apply(weights_init)\n",
    "net_glob.train()\n",
    "#net_glob.load_state_dict(w_glob)\n",
    "net_glob=net_glob.to(dev)\n",
    "net_glob.train()\n",
    "# copy weights\n",
    "#net_glob=torch.hub.load('pytorch/vision:v0.10.0','resnet18',pretrained=False)\n",
    "#net_glob.eval()\n",
    "w_glob = net_glob.state_dict()\n",
    "g, dim = weight_vectorization_gen(w_glob)\n",
    "\n",
    "w_glob=net_glob.state_dict()\n",
    "# print(w_glob)\n",
    "# print(w_glob.keys())\n",
    "for k in w_glob.keys():\n",
    "    print(w_glob[k].shape)\n",
    "net_glob_original=copy.deepcopy(net_glob)\n",
    "net_glob_original.to(dev)\n",
    "w_glob_original=copy.deepcopy(w_glob)\n",
    "w_glob_hold=net_glob_original.state_dict()\n",
    "#print(w_glob_hold)\n",
    "for h in w_glob_hold.keys():\n",
    "    print(w_glob_hold[h].shape)\n",
    "    #print(w_glob[h])\n",
    "    w_glob_hold[h]=torch.zeros(w_glob_hold[h].shape)\n",
    "    print(type(w_glob[h]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(w_glob_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no. 0\n",
      "accuracy array\n",
      "6.960000038146973\n",
      "train loss\n",
      "0.46721555054547914\n",
      "iteration no. 1\n",
      "accuracy array\n",
      "21.1299991607666\n",
      "train loss\n",
      "0.33192093262832495\n",
      "iteration no. 2\n",
      "accuracy array\n",
      "30.350000381469727\n",
      "train loss\n",
      "0.28724842965523695\n",
      "iteration no. 3\n",
      "accuracy array\n",
      "49.060001373291016\n",
      "train loss\n",
      "0.2652151879792279\n",
      "iteration no. 4\n",
      "accuracy array\n",
      "53.709999084472656\n",
      "train loss\n",
      "0.24340683007141017\n",
      "iteration no. 5\n",
      "accuracy array\n",
      "52.83000183105469\n",
      "train loss\n",
      "0.21905086988906902\n",
      "iteration no. 6\n",
      "accuracy array\n",
      "54.810001373291016\n",
      "train loss\n",
      "0.21949581999953854\n",
      "iteration no. 7\n",
      "accuracy array\n",
      "68.45999908447266\n",
      "train loss\n",
      "0.20148748162306104\n",
      "iteration no. 8\n",
      "accuracy array\n",
      "77.83000183105469\n",
      "train loss\n",
      "0.19824826088338227\n",
      "iteration no. 9\n",
      "accuracy array\n",
      "65.1500015258789\n",
      "train loss\n",
      "0.19670546709371095\n",
      "iteration no. 10\n",
      "accuracy array\n",
      "77.55999755859375\n",
      "train loss\n",
      "0.19246396821381292\n",
      "iteration no. 11\n",
      "accuracy array\n",
      "81.44000244140625\n",
      "train loss\n",
      "0.18869615055945085\n",
      "iteration no. 12\n",
      "accuracy array\n",
      "82.16000366210938\n",
      "train loss\n",
      "0.19034239302724723\n",
      "iteration no. 13\n",
      "accuracy array\n",
      "74.87000274658203\n",
      "train loss\n",
      "0.18370319960873516\n",
      "iteration no. 14\n",
      "accuracy array\n",
      "69.63999938964844\n",
      "train loss\n",
      "0.18692441182029035\n",
      "iteration no. 15\n",
      "accuracy array\n",
      "72.97000122070312\n",
      "train loss\n",
      "0.183456533028463\n",
      "iteration no. 16\n",
      "accuracy array\n",
      "72.5999984741211\n",
      "train loss\n",
      "0.16571036698967936\n",
      "iteration no. 17\n",
      "accuracy array\n",
      "77.62999725341797\n",
      "train loss\n",
      "0.17551167029462075\n",
      "iteration no. 18\n",
      "accuracy array\n",
      "58.189998626708984\n",
      "train loss\n",
      "0.1732938220545999\n",
      "iteration no. 19\n",
      "accuracy array\n",
      "81.44999694824219\n",
      "train loss\n",
      "0.16783236471119178\n",
      "iteration no. 20\n",
      "accuracy array\n",
      "82.06999969482422\n",
      "train loss\n",
      "0.16900555630607458\n",
      "iteration no. 21\n",
      "accuracy array\n",
      "69.58000183105469\n",
      "train loss\n",
      "0.1643642781484433\n",
      "iteration no. 22\n",
      "accuracy array\n",
      "74.8499984741211\n",
      "train loss\n",
      "0.15765172432774954\n",
      "iteration no. 23\n",
      "accuracy array\n",
      "74.9800033569336\n",
      "train loss\n",
      "0.16578589793682727\n",
      "iteration no. 24\n",
      "accuracy array\n",
      "74.08000183105469\n",
      "train loss\n",
      "0.15809506382711203\n",
      "iteration no. 25\n",
      "accuracy array\n",
      "70.66999816894531\n",
      "train loss\n",
      "0.160834805553038\n",
      "iteration no. 26\n",
      "accuracy array\n",
      "68.81999969482422\n",
      "train loss\n",
      "0.15510648891927423\n",
      "iteration no. 27\n",
      "accuracy array\n",
      "74.66000366210938\n",
      "train loss\n",
      "0.1638568377607238\n",
      "iteration no. 28\n",
      "accuracy array\n",
      "76.69000244140625\n",
      "train loss\n",
      "0.16196014759605645\n",
      "iteration no. 29\n",
      "accuracy array\n",
      "80.7699966430664\n",
      "train loss\n",
      "0.1558332627532367\n",
      "iteration no. 30\n",
      "accuracy array\n",
      "73.87999725341797\n",
      "train loss\n",
      "0.15402922274179268\n",
      "iteration no. 31\n",
      "accuracy array\n",
      "84.91000366210938\n",
      "train loss\n",
      "0.1511639436943518\n",
      "iteration no. 32\n",
      "accuracy array\n",
      "75.9800033569336\n",
      "train loss\n",
      "0.1493945092976049\n",
      "iteration no. 33\n",
      "accuracy array\n",
      "79.58000183105469\n",
      "train loss\n",
      "0.15212090709898074\n",
      "iteration no. 34\n",
      "accuracy array\n",
      "78.30999755859375\n",
      "train loss\n",
      "0.15514526283095328\n",
      "iteration no. 35\n",
      "accuracy array\n",
      "79.54000091552734\n",
      "train loss\n",
      "0.15705319505861423\n",
      "iteration no. 36\n",
      "accuracy array\n",
      "80.0999984741211\n",
      "train loss\n",
      "0.16215950511170438\n",
      "iteration no. 37\n",
      "accuracy array\n",
      "78.51000213623047\n",
      "train loss\n",
      "0.15883334094451787\n",
      "iteration no. 38\n",
      "accuracy array\n",
      "71.95999908447266\n",
      "train loss\n",
      "0.16430045880471864\n",
      "iteration no. 39\n",
      "accuracy array\n",
      "86.4000015258789\n",
      "train loss\n",
      "0.15753008632633939\n",
      "iteration no. 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training\n",
    "loss_train = []\n",
    "loss_test_arr = []\n",
    "acc_test_arr = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "m_local=[]\n",
    "d=11699132 #resnet18\n",
    "d= 11173962\n",
    "d=11183582\n",
    "d=21840\n",
    "#d= 11699132\n",
    "#d=62006\n",
    "iter_no=2000\n",
    "avg=[]\n",
    "error=[]\n",
    "idxs_users=range(0,args.num_users)\n",
    "    #print(len(idxs_users))\n",
    "user_no=args.num_users\n",
    "updated=[]\n",
    "a=[]\n",
    "for user in idxs_users:\n",
    "        #print(user)\n",
    "    updated.append([])  \n",
    "    a.append([])   \n",
    "model_diff=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#net_glob.zero_grad()\n",
    "input_gradient=[]\n",
    "\n",
    "user_no=args.num_users\n",
    "\n",
    "select=round(0.9*user_no)\n",
    "\n",
    "K_local=round(1*d)\n",
    "loss_train=[]    \n",
    "#net_glob.zero_grad()   \n",
    "for iter in range(iter_no): #args.epochs\n",
    "    print(\"iteration no.\",iter)\n",
    "    if (iter>=500):\n",
    "        args.lr=args.lr/5\n",
    "    m_local=[]\n",
    "    f=[]\n",
    "        #T=[]\n",
    "    w_locals, loss_locals,diff_locals,grad_locals = [], [],[],[]\n",
    "    m = 10\n",
    "    updated=[]\n",
    "    model_diff=[]\n",
    "    grad_vect=[]\n",
    "    prev=[]\n",
    "    error=[]\n",
    "    grad_vect_quant=[]\n",
    "    grad_vect_quant2=[]\n",
    "    grad_vect_send=[]\n",
    "    grad_vect_send2=[]\n",
    "    store_grad=[]\n",
    "    location_local=[]\n",
    "    rand=np.random.choice(idxs_users,user_no,replace=False)\n",
    "    rand=idxs_users\n",
    "    np.random.seed(iter)\n",
    "    rand=np.random.choice(idxs_users, select, replace=False)\n",
    "    rand=np.sort(rand)\n",
    "    loss_train_user=[]\n",
    "    for i in range(args.num_users):\n",
    "        updated.append([])\n",
    "        model_diff.append([])\n",
    "        grad_vect.append([])\n",
    "        prev.append([])\n",
    "        grad_vect_send.append([])\n",
    "        error.append(np.zeros(d))\n",
    "        grad_vect_quant.append([])\n",
    "    \n",
    "    \n",
    "    for user in rand : #rand: #idxs_users:\n",
    "        #print(user)\n",
    "        \n",
    "        w_glob = net_glob_original.state_dict()\n",
    "        my_list=[0,1,2,3]\n",
    "        block=int(comp[user]*4)\n",
    "        sub=[]\n",
    "        for i in range(block):\n",
    "            sub.append(random.choice(my_list))\n",
    "            my_list.remove(sub[i])\n",
    "       \n",
    "        #s=s_1[user]\n",
    "        sub=sorted(sub)\n",
    "#print(w_glob['layer_input.weight'][:,s:s+10])\n",
    "        \n",
    "\n",
    "        y=int(comp[user]*200)\n",
    "        \n",
    "        count=0\n",
    "        for i in sub:\n",
    "            first=w_glob['layer_input.weight'][int(50*i):int(50*(i+1)),:]\n",
    "            #print(first.shape)\n",
    "            if (count==0):\n",
    "                combined=first\n",
    "            else:\n",
    "                first=w_glob['layer_input.weight'][int(50*i):int(50*(i+1)),:]\n",
    "                combined=torch.concatenate((combined,first),axis=0)\n",
    "            count+=1\n",
    "            #print(combined.shape)\n",
    "    \n",
    "        w_glob['layer_input.weight']=combined\n",
    "        count=0\n",
    "        for i in sub:\n",
    "            first=w_glob['layer_input.bias'][int(50*i):int(50*(i+1))]\n",
    "            if (count==0):\n",
    "                combined=first\n",
    "            else:\n",
    "                first=w_glob['layer_input.bias'][int(50*i):int(50*(i+1))]\n",
    "                combined=torch.concatenate((combined,first),axis=0)\n",
    "            count+=1\n",
    "    \n",
    "        w_glob['layer_input.bias']=combined\n",
    "        \n",
    "        count=0\n",
    "        for i in sub:\n",
    "            first=w_glob['layer_hidden.weight'][:,int(50*i):int(50*(i+1))]\n",
    "            if (count==0):\n",
    "                combined=first\n",
    "            else:\n",
    "                first=w_glob['layer_hidden.weight'][:,int(50*i):int(50*(i+1))]\n",
    "                combined=torch.concatenate((combined,first),axis=1)\n",
    "            count+=1\n",
    "        w_glob['layer_hidden.weight']=combined\n",
    "        \n",
    "        \n",
    "        #print(w_glob)\n",
    "        \n",
    "        net_glob = MLP(dim_in=len_in, dim_hidden=y, dim_out=args.num_classes).to(args.device)\n",
    "        net_glob.train()\n",
    "        net_glob.to(dev)\n",
    "        \n",
    "        net_glob.load_state_dict(w_glob)\n",
    "        #s_1[user]=(s_1[user]+1)%200\n",
    "        \n",
    "        prev[user]=copy.deepcopy(w_glob)\n",
    "        local = LocalTrain(args=args, dataset=dataset_train, idxs=dict_users[user])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "#         input_gradient=inversefed.reconstruction_algorithms.loss_steps(net_glob, ground_truth[user], labels[user], \n",
    "#                                                         lr=local_lr, local_steps=local_steps,\n",
    "#                                                                    use_updates=use_updates)\n",
    "        \n",
    "        loss_train_user.append(loss)\n",
    "    \n",
    "        model_diff=FedSubstract(w,prev[user])\n",
    "        #print(model_diff)\n",
    "        \n",
    "        g, dim = weight_vectorization_gen(model_diff)\n",
    "        #print(len(g))\n",
    "        \n",
    "        model_diff=update_local_model(w_glob_hold, model_diff, sub)\n",
    "        #print(model_diff2)\n",
    "        \n",
    "        g, dim = weight_vectorization_gen(model_diff)\n",
    "        #print(len(g))\n",
    "        g=g[:,0] #+error[user]\n",
    "        grad1=abs(np.array(g))\n",
    "        #location_local=np.random.choice(range(d),K_local,replace=False)\n",
    "        #location_local=np.argpartition(grad1,-K_local)[-K_local:]\n",
    "        #a[user].append(np.sort(location_local))\n",
    "        #if iter>0:\n",
    "            #print(len(set(a[user][iter-1]).intersection(set(a[user][iter]))))\n",
    "        #print(a)\n",
    "#         mask=np.zeros(d)\n",
    "#         np.put(mask,location_local,1)\n",
    "        grad_vect[user]= g #np.multiply(g,mask)\n",
    "        scale=1/(1*0.9*args.num_users)\n",
    "        grad_vect_quant[user]= phiQ(np.power(2,args.f_size),scale,2**args.q,grad_vect[user])\n",
    "        #error[user]=g-grad_vect[user]\n",
    "        grad_locals.append(grad_vect_quant[user])\n",
    "        del g\n",
    "        #print(user)\n",
    "    grad_avg=sum(grad_locals) #/len(grad_locals)\n",
    "    loss_train.append(sum(loss_train_user)/len(loss_train_user))\n",
    "    grad_avg=sum(grad_locals) # taking the average of masked gradients\n",
    "    #grad_avg2=np.nan_to_num(sum(grad_locals2))\n",
    "    #print(\"grad_avg\")\n",
    "    #print(grad_avg)\n",
    "    grad_avg_correct = np.zeros_like(grad_avg)\n",
    "    #print(\"before modulo\")\n",
    "    #print(grad_avg)\n",
    "    grad_avg= (grad_avg)%(np.power(2,args.f_size)-5)\n",
    "    #print(\"after modulo\")\n",
    "    #print(grad_avg)\n",
    "    p=np.power(2,args.f_size)-5\n",
    "    for i in range(len(grad_avg)):\n",
    "        if grad_avg[i]>=0 and grad_avg[i]<(p-1)/2:\n",
    "                        # print(\"Valid\")\n",
    "            grad_avg_correct[i]=grad_avg[i]\n",
    "            grad_avg_correct[i]=(1/(2**args.q))*grad_avg_correct[i]\n",
    "            #grad_avg_correct[i]=(1/(2**args.q))*grad_avg_correct[i]\n",
    "            continue\n",
    "        elif grad_avg[i]>=(p-1)/2 and grad_avg[i]<p:\n",
    "                        # print(\"Chenged\")\n",
    "            grad_avg_correct[i]=grad_avg[i]-p\n",
    "            grad_avg_correct[i]=(1/(2**args.q))*grad_avg_correct[i]\n",
    "    count=0\n",
    "    w_glob_prev=copy.deepcopy(w_glob_original)\n",
    "    flat=[]\n",
    "    #conver\n",
    "    for i in range(len(w_glob.keys())): # 4 layers in parameter\n",
    "        flat.append([])\n",
    "\n",
    "    for h in w_glob_prev.keys():\n",
    "        s=list(w_glob_original[h].shape)\n",
    "        if (len(s)==0):\n",
    "            new=np.array(0)\n",
    "            grad_avg_correct=np.delete(grad_avg_correct,np.s_[0])\n",
    "        else:\n",
    "            z=np.prod(list(w_glob_original[h].shape))\n",
    "            flat[count]=grad_avg_correct[0:z] # taking out the vector for the specified layer\n",
    "            grad_avg_correct=np.delete(grad_avg_correct,np.s_[0:z]) # deleting that vector from decoded after taking out\n",
    "             \n",
    "            new=flat[count].reshape(list(w_glob_original[h].shape)) #reshaping back to the marix\n",
    "              \n",
    "        w_glob_original[h]=torch.from_numpy(new) #converting the matrix to a tensor\n",
    "            #print(w_glob[cluster_no][h].shape)\n",
    "        count=count+1\n",
    "    global_diff = w_glob_original\n",
    "    #print(w_glob)\n",
    "    w_glob_original=FedAdd(w_glob_prev,global_diff)\n",
    "    # update global weights\n",
    "    #global_diff = w_glob\n",
    "    #print(w_glob)\n",
    "    #w_glob=FedAdd(w_glob_prev,global_diff)\n",
    "    \n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob_original.load_state_dict(w_glob_original)\n",
    "    \n",
    "    del w_glob_prev\n",
    "    del grad_locals\n",
    "    del grad_avg\n",
    "    del flat\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # print loss\n",
    "    #loss_avg = np.nan_to_num(sum(loss_locals) / len(loss_locals))\n",
    "    \n",
    "    #loss_train.append(float(loss_avg))\n",
    "    \n",
    "    acc_test, loss_test = test_acc(net_glob, dataset_test, args)\n",
    "    acc_test_arr.append(float(acc_test))\n",
    "    loss_test_arr.append(loss_test)\n",
    "    if iter % 1 ==0:\n",
    "        #print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    " \n",
    "        print(\"accuracy array\")\n",
    "        print(acc_test_arr[iter])\n",
    "        print(\"train loss\")\n",
    "        print(loss_train[iter])       \n",
    "        \n",
    "        \n",
    "   \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_diff['layer_hidden.weight'])\n",
    "#print(model_diff2['layer_hidden.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "step=1500\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "matplotlib.rc('xtick', labelsize=18) \n",
    "matplotlib.rc('ytick', labelsize=18) \n",
    "#plt.plot(range(len(Cluster0_minor1)), Cluster0_minor1,label=\"users=2,5,5,6,7\")\n",
    "plt.plot(range(len(acc_test_arr[0:step])), acc_test_arr[0:step],label=\"without sparsification\")\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.xlabel('# Global Rounds')\n",
    "#plt.legend()\n",
    "#plt.figure(figsize=(6,5), dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net_glob.state_dict(),\"model_cnn.pt\")\n",
    "#for k in w_glob.keys():\n",
    "#print(w_glob['conv1.weight'].to(torch.device(\"cuda\"))-w_glob['conv1.weight'].to(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((w_glob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
